{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Ht65TfR5Ss"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOymWXyiSYUQ"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82flzghiSeVT"
      },
      "outputs": [],
      "source": [
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGdaa0z6TU78"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Fbb9cETlNb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import os, time\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOCMBCeFT8on"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "data_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/My Drive/Virat/cropped_train',\n",
        "    target_size = (32,32),\n",
        "    batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvc1trgrVQ9-"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels = train_generator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeHpg7dqVpap"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLra6bNlWyaL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APpTHEjzXPMD"
      },
      "outputs": [],
      "source": [
        "img_shape = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQGKAbEjXh7T"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h5JTJK5n9o7"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8 * 8 * 256, activation = 'relu', input_dim=100))\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2DTranspose(128, kernel_size = 4, strides = 2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2DTranspose(64, kernel_size = 4, strides = 2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2DTranspose(3, kernel_size = 4, strides = 1, padding='same', activation = 'tanh'))\n",
        "    return model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size = 4, strides = 2, padding='same', input_shape = img_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(128, kernel_size = 4, strides = 2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzcQCBxkrUJK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "generator = build_generator()\n",
        "\n",
        "z = tf.keras.Input(shape=(100,))\n",
        "img = generator(z)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.trainable = False\n",
        "valid_pred = discriminator(img)\n",
        "\n",
        "combined = tf.keras.Model(z, valid_pred)\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "combined.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2A_mNatsRMw"
      },
      "outputs": [],
      "source": [
        "def train(epochs, batch_size, save_interval = 30):\n",
        "    X_train = next(train_generator)[0]\n",
        "    x_train = (X_train - 0.5) * 2\n",
        "    min_batch = int(batch_size / 2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        id = np.random.randint(0, X_train.shape[0], min_batch)\n",
        "        x_real = x_train[id]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (min_batch, 100))\n",
        "        x_fake = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(x_real, np.ones((min_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(x_fake, np.zeros((min_batch, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        if isinstance(d_loss_real, (list, tuple)):\n",
        "            d_loss_real_val = d_loss_real[0]\n",
        "            d_acc_real = d_loss_real[1]\n",
        "        else:\n",
        "            d_loss_real_val = d_loss_real\n",
        "            d_acc_real = 0\n",
        "\n",
        "        if isinstance(d_loss_fake, (list, tuple)):\n",
        "            d_loss_fake_val = d_loss_fake[0]\n",
        "            d_acc_fake = d_loss_fake[1]\n",
        "        else:\n",
        "            d_loss_fake_val = d_loss_fake\n",
        "            d_acc_fake = 0\n",
        "\n",
        "        d_loss = 0.5 * (d_loss_real_val + d_loss_fake_val)\n",
        "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "\n",
        "        noise = np.random.normal(0, 1, (min_batch, 100))\n",
        "        valid_y = np.ones((min_batch, 1))\n",
        "        g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "        if isinstance(g_loss, (list, tuple)):\n",
        "            g_loss_val = g_loss[0]\n",
        "        else:\n",
        "            g_loss_val = g_loss\n",
        "        print(f\"{epoch} [D loss: {d_loss}, acc.: {100*d_acc}] [G loss: {g_loss_val}]\")\n",
        "\n",
        "        if epoch % save_interval == 0:\n",
        "            print(f\"Saving images at epoch {epoch}\")\n",
        "            save_imgs(epoch)\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    noise = np.random.normal(0, 1, (16, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4)\n",
        "    count = 0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i,j].imshow(gen_imgs[count])\n",
        "            axs[i,j].axis('off')\n",
        "            count += 1\n",
        "    save_dir = \"/content/drive/My Drive/Virat/Output\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig.savefig(os.path.join(save_dir, 'image_at_epoch_{:04d}.png'.format(epoch)))\n",
        "    plt.close()\n",
        "    print(f\"Image saved for epoch {epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1tyyN5c2iLA"
      },
      "outputs": [],
      "source": [
        "train(epochs=5001, batch_size=32, save_interval = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1PQDu9t3hQz"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "image_files = sorted([f for f in os.listdir('/content/drive/My Drive/Virat/Output') if f.endswith('.png')])\n",
        "\n",
        "\n",
        "with imageio.get_writer('/content/drive/My Drive/Virat/Output/gan.gif', mode='I') as writer:\n",
        "    for filename in image_files:\n",
        "        image = Image.open(os.path.join('/content/drive/My Drive/Virat/Output', filename))\n",
        "        writer.append_data(np.array(image))\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename='/content/drive/My Drive/Virat/Output/gan.gif')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdPJAufNX1xB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}